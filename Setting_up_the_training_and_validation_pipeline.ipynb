{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Setting up the training and validation pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interviewBubble/TensorFlow-Tutorial/blob/master/Setting_up_the_training_and_validation_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E18CJmBJLZd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In the previous session we trained a model without keeping track of how it's\n",
        "doing on a validation set. Let's pick up where we left off and modify our code\n",
        "from the previous session to keep track of validation accuracy while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTKzIRuOJEdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0ygoC4kJslr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(im, label):\n",
        "    im = tf.cast(im, tf.float32)\n",
        "    im = im / 127.5\n",
        "    im = im - 1\n",
        "    im = tf.reshape(im, [-1])\n",
        "    return im, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zThOdGhcJx8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will be using the same data pipeline for both training and validation sets\n",
        "# So let's create a helper function for that\n",
        "def create_dataset_pipeline(data_tensor, is_train=True, num_threads=8, prefetch_buffer=100, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tensor)\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(buffer_size=60000).repeat()\n",
        "    dataset = dataset.map(preprocess_data, num_parallel_calls=num_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(prefetch_buffer)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkRux0F7J3Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_layer():\n",
        "    with tf.variable_scope(\"data\"):\n",
        "        data_train, data_val = tf.keras.datasets.mnist.load_data()\n",
        "        dataset_train = create_dataset_pipeline(data_train, is_train=True)\n",
        "        dataset_val = create_dataset_pipeline(data_val, is_train=False, batch_size=1)\n",
        "        iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n",
        "        init_op_train = iterator.make_initializer(dataset_train)\n",
        "        init_op_val = iterator.make_initializer(dataset_val)\n",
        "    return iterator, init_op_train, init_op_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoD3ouXJ67R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(input_layer, num_classes=10):\n",
        "    with tf.variable_scope(\"model\"):\n",
        "        net = tf.layers.dense(input_layer, 512)\n",
        "        net = tf.nn.relu(net)\n",
        "        net = tf.layers.dense(net, num_classes)\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbNuTyj-KEMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_functions(logits, labels, num_classes=10):\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "        target_prob = tf.one_hot(labels, num_classes)\n",
        "        total_loss = tf.losses.softmax_cross_entropy(target_prob, logits)\n",
        "    return total_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcxTx6Y1KFaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimizer_func(total_loss, global_step, learning_rate=0.1):\n",
        "    with tf.variable_scope(\"optimizer\"):\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        optimizer = optimizer.minimize(total_loss, global_step=global_step)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8IFLMhKKHIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_metric(logits, labels):\n",
        "    with tf.variable_scope(\"performance_metric\"):\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        labels = tf.cast(labels, tf.int64)\n",
        "        corrects = tf.equal(preds, labels)\n",
        "        accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wcH0PpKKL6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    global_step = tf.Variable(1, dtype=tf.int32, trainable=False, name=\"iter_number\")\n",
        "\n",
        "    # define the training graph\n",
        "    iterator, init_op_train, init_op_val = data_layer()\n",
        "    images, labels = iterator.get_next()\n",
        "    logits = model(images)\n",
        "    loss = loss_functions(logits, labels)\n",
        "    optimizer = optimizer_func(loss, global_step)\n",
        "    accuracy = performance_metric(logits, labels)\n",
        "\n",
        "    # start training\n",
        "    num_iter = 18750 # 10 epochs\n",
        "    log_iter = 1875\n",
        "    val_iter = 1875\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(init_op_train)\n",
        "\n",
        "        streaming_loss = 0\n",
        "        streaming_accuracy = 0\n",
        "\n",
        "        for i in range(1, num_iter + 1):\n",
        "            _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy])\n",
        "            streaming_loss += loss_batch\n",
        "            streaming_accuracy += acc_batch\n",
        "            if i % log_iter == 0:\n",
        "                print(\"Iteration: {}, Streaming loss: {:.2f}, Streaming accuracy: {:.2f}\"\n",
        "                        .format(i, streaming_loss/log_iter, streaming_accuracy/log_iter))\n",
        "                streaming_loss = 0\n",
        "                streaming_accuracy = 0\n",
        "\n",
        "            if i % val_iter == 0:\n",
        "                sess.run(init_op_val)\n",
        "                validation_accuracy = 0\n",
        "                num_iter = 0\n",
        "                while True:\n",
        "                    try:\n",
        "                        acc_batch = sess.run(accuracy)\n",
        "                        validation_accuracy += acc_batch\n",
        "                        num_iter += 1\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        validation_accuracy /= num_iter\n",
        "                        print(\"Iteration: {}, Validation accuracy: {:.2f}\".format(i, validation_accuracy))\n",
        "                        sess.run(init_op_train) # switch back to training set\n",
        "                        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cJbacOiK0s6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "aeb2328b-3dcc-409b-fc6b-4f506e636763"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From <ipython-input-5-385b01cc5114>:6: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
            "WARNING:tensorflow:From <ipython-input-5-385b01cc5114>:6: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "WARNING:tensorflow:From <ipython-input-6-0c9f91e3d35d>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Iteration: 1875, Streaming loss: 0.32, Streaming accuracy: 0.90\n",
            "Iteration: 1875, Validation accuracy: 0.96\n",
            "Iteration: 3750, Streaming loss: 0.14, Streaming accuracy: 0.96\n",
            "Iteration: 3750, Validation accuracy: 0.97\n",
            "Iteration: 5625, Streaming loss: 0.10, Streaming accuracy: 0.97\n",
            "Iteration: 5625, Validation accuracy: 0.97\n",
            "Iteration: 7500, Streaming loss: 0.08, Streaming accuracy: 0.97\n",
            "Iteration: 7500, Validation accuracy: 0.97\n",
            "Iteration: 9375, Streaming loss: 0.07, Streaming accuracy: 0.98\n",
            "Iteration: 9375, Validation accuracy: 0.97\n",
            "Iteration: 11250, Streaming loss: 0.06, Streaming accuracy: 0.98\n",
            "Iteration: 11250, Validation accuracy: 0.98\n",
            "Iteration: 13125, Streaming loss: 0.05, Streaming accuracy: 0.98\n",
            "Iteration: 13125, Validation accuracy: 0.98\n",
            "Iteration: 15000, Streaming loss: 0.04, Streaming accuracy: 0.99\n",
            "Iteration: 15000, Validation accuracy: 0.97\n",
            "Iteration: 16875, Streaming loss: 0.03, Streaming accuracy: 0.99\n",
            "Iteration: 16875, Validation accuracy: 0.98\n",
            "Iteration: 18750, Streaming loss: 0.03, Streaming accuracy: 0.99\n",
            "Iteration: 18750, Validation accuracy: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZivYtLpmK6eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}