{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization, saving and resuming from checkpoints, and TensorBoard.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interviewBubble/TensorFlow-Tutorial/blob/master/Regularization%2C_saving_and_resuming_from_checkpoints%2C_and_TensorBoard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bma4CQJknApT",
        "colab_type": "text"
      },
      "source": [
        "In the previous session, we wrote this code to train a simple model in TensorFlow.\n",
        "In this session, we will train a deeper model, regularize it, and visualize it in TensorBoard.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz-dw9gpmy6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPeeEpMBnCvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(im, label):\n",
        "    im = tf.cast(im, tf.float32)\n",
        "    im = im / 127.5\n",
        "    im = im - 1\n",
        "    im = tf.reshape(im, [-1])\n",
        "    return im, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joFs2QGLnWx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will be using the same data pipeline for both training and validation sets\n",
        "# So let's create a helper function for that\n",
        "def create_dataset_pipeline(data_tensor, is_train=True, num_threads=8, prefetch_buffer=100, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tensor)\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(buffer_size=60000).repeat()\n",
        "    dataset = dataset.map(preprocess_data, num_parallel_calls=num_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(prefetch_buffer)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFl38C6rnbID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_layer():\n",
        "    with tf.variable_scope(\"data\"):\n",
        "        data_train, data_val = tf.keras.datasets.mnist.load_data()\n",
        "        dataset_train = create_dataset_pipeline(data_train, is_train=True)\n",
        "        dataset_val = create_dataset_pipeline(data_val, is_train=False, batch_size=1)\n",
        "        iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n",
        "        init_op_train = iterator.make_initializer(dataset_train)\n",
        "        init_op_val = iterator.make_initializer(dataset_val)\n",
        "    return iterator, init_op_train, init_op_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HApwFX4zneb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(input_layer, num_classes=10):\n",
        "    with tf.variable_scope(\"model\"):\n",
        "        reg = tf.contrib.layers.l2_regularizer(0.00001)\n",
        "        net = input_layer\n",
        "        for i in range(3):\n",
        "            net = tf.layers.dense(net,\n",
        "                                units=512,\n",
        "                                kernel_regularizer=reg)\n",
        "            net = tf.nn.relu(net)\n",
        "            net = tf.layers.dropout(net, rate=0.2)\n",
        "        net = tf.layers.dense(net, num_classes)\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-7jHWOVr_ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_functions(logits, labels, num_classes=10):\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "        target_prob = tf.one_hot(labels, num_classes)\n",
        "        tf.losses.softmax_cross_entropy(target_prob, logits)\n",
        "        total_loss = tf.losses.get_total_loss() # include regularization loss (I forgot to add this in the video)\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1KCmUkksFGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimizer_func(total_loss, global_step, learning_rate=0.1):\n",
        "    with tf.variable_scope(\"optimizer\"):\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        optimizer = optimizer.minimize(total_loss, global_step=global_step)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRfmPjt2sHk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_metric(logits, labels):\n",
        "    with tf.variable_scope(\"performance_metric\"):\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        labels = tf.cast(labels, tf.int64)\n",
        "        corrects = tf.equal(preds, labels)\n",
        "        accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3xuleKnsKiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    global_step = tf.Variable(1, dtype=tf.int32, trainable=False, name=\"iter_number\")\n",
        "\n",
        "    # define the training graph\n",
        "    iterator, init_op_train, init_op_val = data_layer()\n",
        "    images, labels = iterator.get_next()\n",
        "    logits = model(images)\n",
        "    loss = loss_functions(logits, labels)\n",
        "    optimizer = optimizer_func(loss, global_step)\n",
        "    accuracy = performance_metric(logits, labels)\n",
        "\n",
        "    ########################################################################\n",
        "    # summary placeholders\n",
        "    streaming_loss_p = tf.placeholder(tf.float32)\n",
        "    streaming_acc_p = tf.placeholder(tf.float32)\n",
        "    val_acc_p = tf.placeholder(tf.float32)\n",
        "    val_summ_ops = tf.summary.scalar('validation_acc', val_acc_p)\n",
        "    train_summ_ops = tf.summary.merge([\n",
        "        tf.summary.scalar('streaming_loss', streaming_loss_p),\n",
        "        tf.summary.scalar('streaming_accuracy', streaming_acc_p)\n",
        "    ])\n",
        "    ########################################################################\n",
        "\n",
        "    # start training\n",
        "    num_iter = 18750 # 10 epochs\n",
        "    log_iter = 1875\n",
        "    val_iter = 1875\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(init_op_train)\n",
        "\n",
        "        ########################################################################\n",
        "        # logs for TensorBoard\n",
        "        logdir = 'logs'\n",
        "        writer = tf.summary.FileWriter(logdir, sess.graph) # visualize the graph\n",
        "\n",
        "        # load / save checkpoints\n",
        "        checkpoint_path = 'checkpoints'\n",
        "        saver = tf.train.Saver(max_to_keep=None)\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
        "\n",
        "        # resume training if a checkpoint exists\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            print(\"Loaded parameters from {}\".format(ckpt.model_checkpoint_path))\n",
        "\n",
        "        initial_step = global_step.eval()\n",
        "        ########################################################################\n",
        "\n",
        "        streaming_loss = 0\n",
        "        streaming_accuracy = 0\n",
        "\n",
        "        for i in range(initial_step, num_iter + 1): #################################### initial step\n",
        "            _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy])\n",
        "            streaming_loss += loss_batch\n",
        "            streaming_accuracy += acc_batch\n",
        "            if i % log_iter == 0:\n",
        "                print(\"Iteration: {}, Streaming loss: {:.2f}, Streaming accuracy: {:.2f}\"\n",
        "                        .format(i, streaming_loss/log_iter, streaming_accuracy/log_iter))\n",
        "\n",
        "                #####################################################################################\n",
        "                # save to log file for TensorBoard\n",
        "                summary_train = sess.run(train_summ_ops, feed_dict={streaming_loss_p: streaming_loss,\n",
        "                                                                    streaming_acc_p: streaming_accuracy})\n",
        "                writer.add_summary(summary_train, global_step=i)\n",
        "                #####################################################################################\n",
        "\n",
        "                streaming_loss = 0\n",
        "                streaming_accuracy = 0\n",
        "\n",
        "            if i % val_iter == 0:\n",
        "                #####################################################################################\n",
        "                saver.save(sess, os.path.join(checkpoint_path, 'checkpoint'), global_step=global_step)\n",
        "                print(\"Model saved!\")\n",
        "                #####################################################################################\n",
        "\n",
        "                sess.run(init_op_val)\n",
        "                validation_accuracy = 0\n",
        "                num_iter = 0\n",
        "                while True:\n",
        "                    try:\n",
        "                        acc_batch = sess.run(accuracy)\n",
        "                        validation_accuracy += acc_batch\n",
        "                        num_iter += 1\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        validation_accuracy /= num_iter\n",
        "                        print(\"Iteration: {}, Validation accuracy: {:.2f}\".format(i, validation_accuracy))\n",
        "\n",
        "                        ###############################################################################\n",
        "                        # save log file to TensorBoard\n",
        "                        summary_val = sess.run(val_summ_ops, feed_dict={val_acc_p: validation_accuracy})\n",
        "                        writer.add_summary(summary_val, global_step=i)\n",
        "                        ###############################################################################\n",
        "\n",
        "                        sess.run(init_op_train) # switch back to training set\n",
        "                        break\n",
        "        writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjsUsOx9sPWh",
        "colab_type": "code",
        "outputId": "51278117-b6ee-4659-bcaa-d68bc96fa19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tf.reset_default_graph()\n",
        "    train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from checkpoints/checkpoint-3751\n",
            "Loaded parameters from checkpoints/checkpoint-3751\n",
            "Iteration: 5625, Streaming loss: 0.10, Streaming accuracy: 0.97\n",
            "INFO:tensorflow:checkpoints/checkpoint-5626 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 5625, Validation accuracy: 0.97\n",
            "Iteration: 7500, Streaming loss: 0.08, Streaming accuracy: 0.98\n",
            "INFO:tensorflow:checkpoints/checkpoint-7501 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 7500, Validation accuracy: 0.97\n",
            "Iteration: 9375, Streaming loss: 0.06, Streaming accuracy: 0.98\n",
            "INFO:tensorflow:checkpoints/checkpoint-9376 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 9375, Validation accuracy: 0.97\n",
            "Iteration: 11250, Streaming loss: 0.05, Streaming accuracy: 0.99\n",
            "INFO:tensorflow:checkpoints/checkpoint-11251 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 11250, Validation accuracy: 0.98\n",
            "Iteration: 13125, Streaming loss: 0.04, Streaming accuracy: 0.99\n",
            "INFO:tensorflow:checkpoints/checkpoint-13126 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 13125, Validation accuracy: 0.98\n",
            "Iteration: 15000, Streaming loss: 0.04, Streaming accuracy: 0.99\n",
            "INFO:tensorflow:checkpoints/checkpoint-15001 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 15000, Validation accuracy: 0.97\n",
            "Iteration: 16875, Streaming loss: 0.03, Streaming accuracy: 0.99\n",
            "INFO:tensorflow:checkpoints/checkpoint-16876 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 16875, Validation accuracy: 0.98\n",
            "Iteration: 18750, Streaming loss: 0.03, Streaming accuracy: 0.99\n",
            "INFO:tensorflow:checkpoints/checkpoint-18751 is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Model saved!\n",
            "Iteration: 18750, Validation accuracy: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtvnkxYcsoSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}