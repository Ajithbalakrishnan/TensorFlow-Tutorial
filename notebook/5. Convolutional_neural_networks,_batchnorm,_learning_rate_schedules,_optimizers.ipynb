{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional neural networks, batchnorm, learning rate schedules, optimizers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interviewBubble/TensorFlow-Tutorial/blob/master/convolutional_neural_networks%2C_batchnorm%2C_learning_rate_schedules%2C_optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TVRH0Np8WNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueCMx4xFAkEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(im, label):\n",
        "    im = tf.cast(im, tf.float32)\n",
        "    im = im / 127.5\n",
        "    im = im - 1\n",
        "    # im = tf.reshape(im, [-1])\n",
        "    return im, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDX22yUsAno5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will be using the same data pipeline for both training and validation sets\n",
        "# So let's create a helper function for that\n",
        "def create_dataset_pipeline(data_tensor, is_train=True, num_threads=8, prefetch_buffer=100, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tensor)\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(buffer_size=60000).repeat()\n",
        "    dataset = dataset.map(preprocess_data, num_parallel_calls=num_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(prefetch_buffer)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCTVuYlzAr2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_layer():\n",
        "    with tf.variable_scope(\"data\"):\n",
        "        data_train, data_val = tf.keras.datasets.mnist.load_data()\n",
        "        dataset_train = create_dataset_pipeline(data_train, is_train=True)\n",
        "        dataset_val = create_dataset_pipeline(data_val, is_train=False, batch_size=1)\n",
        "        iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n",
        "        init_op_train = iterator.make_initializer(dataset_train)\n",
        "        init_op_val = iterator.make_initializer(dataset_val)\n",
        "    return iterator, init_op_train, init_op_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB7nwLqKAugm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(input_layer, training, num_classes=10):\n",
        "    with tf.variable_scope(\"model\"):\n",
        "        net = tf.expand_dims(input_layer, axis=3)\n",
        "\n",
        "        net = tf.layers.conv2d(net, 20, (5, 5))\n",
        "        net = tf.layers.batch_normalization(net, training=training)\n",
        "        net = tf.nn.relu(net)\n",
        "        net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(2, 2))\n",
        "\n",
        "        net = tf.layers.conv2d(net, 50, (5, 5))\n",
        "        net = tf.layers.batch_normalization(net, training=training)\n",
        "        net = tf.nn.relu(net)\n",
        "        net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(2, 2))\n",
        "\n",
        "        net = tf.layers.flatten(net)\n",
        "        net = tf.layers.dense(net, 500)\n",
        "        net = tf.nn.relu(net) # I forgot to add this ReLU in the video\n",
        "        net = tf.layers.dropout(net, rate=0.2, training=training) # I forgot the training argument in the video\n",
        "        net = tf.layers.dense(net, num_classes)\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6M3f3RAUZP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_functions(logits, labels, num_classes=10):\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "        target_prob = tf.one_hot(labels, num_classes)\n",
        "        tf.losses.softmax_cross_entropy(target_prob, logits)\n",
        "        total_loss = tf.losses.get_total_loss() # include regularization loss\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65qameoKUcT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimizer_func_momentum(total_loss, global_step, learning_rate=0.01):\n",
        "    with tf.variable_scope(\"optimizer\"):\n",
        "        lr_schedule = tf.train.exponential_decay(learning_rate=learning_rate,\n",
        "                                                 global_step=global_step,\n",
        "                                                 decay_steps=1875,\n",
        "                                                 decay_rate=0.9,\n",
        "                                                 staircase=True)\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            optimizer = tf.train.MomentumOptimizer(learning_rate=lr_schedule, momentum=0.9)\n",
        "            optimizer = optimizer.minimize(total_loss, global_step=global_step)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Gx0kN_Uf2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimizer_func_adam(total_loss, global_step, learning_rate=0.01):\n",
        "    with tf.variable_scope(\"optimizer\"):\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
        "            optimizer = optimizer.minimize(total_loss, global_step=global_step)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ITbQ2pUjCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_metric(logits, labels):\n",
        "    with tf.variable_scope(\"performance_metric\"):\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        labels = tf.cast(labels, tf.int64)\n",
        "        corrects = tf.equal(preds, labels)\n",
        "        accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoAl_-IOUsxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    global_step = tf.Variable(1, dtype=tf.int32, trainable=False, name=\"iter_number\")\n",
        "\n",
        "    # define the training graph\n",
        "    iterator, init_op_train, init_op_val = data_layer()\n",
        "    images, labels = iterator.get_next()\n",
        "    training = tf.placeholder(tf.bool)\n",
        "    logits = model(images, training) ##############################\n",
        "    loss = loss_functions(logits, labels)\n",
        "    optimizer = optimizer_func_adam(loss, global_step) ##############################\n",
        "    accuracy = performance_metric(logits, labels)\n",
        "\n",
        "    # summary placeholders\n",
        "    streaming_loss_p = tf.placeholder(tf.float32)\n",
        "    streaming_acc_p = tf.placeholder(tf.float32)\n",
        "    val_acc_p = tf.placeholder(tf.float32)\n",
        "    val_summ_ops = tf.summary.scalar('validation_acc', val_acc_p)\n",
        "    train_summ_ops = tf.summary.merge([\n",
        "        tf.summary.scalar('streaming_loss', streaming_loss_p),\n",
        "        tf.summary.scalar('streaming_accuracy', streaming_acc_p)\n",
        "    ])\n",
        "\n",
        "    # start training\n",
        "    num_iter = 18750 # 10 epochs\n",
        "    log_iter = 1875\n",
        "    val_iter = 1875\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(init_op_train)\n",
        "\n",
        "        # logs for TensorBoard\n",
        "        logdir = 'logs'\n",
        "        writer = tf.summary.FileWriter(logdir, sess.graph) # visualize the graph\n",
        "\n",
        "        # load / save checkpoints\n",
        "        checkpoint_path = 'checkpoints'\n",
        "        saver = tf.train.Saver(max_to_keep=None)\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
        "\n",
        "        # resume training if a checkpoint exists\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            print(\"Loaded parameters from {}\".format(ckpt.model_checkpoint_path))\n",
        "\n",
        "        initial_step = global_step.eval()\n",
        "\n",
        "        streaming_loss = 0\n",
        "        streaming_accuracy = 0\n",
        "\n",
        "        for i in range(initial_step, num_iter + 1):\n",
        "            _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy], feed_dict={training: True}) ##############################\n",
        "            streaming_loss += loss_batch\n",
        "            streaming_accuracy += acc_batch\n",
        "            if i % log_iter == 0:\n",
        "                print(\"Iteration: {}, Streaming loss: {:.2f}, Streaming accuracy: {:.2f}\"\n",
        "                        .format(i, streaming_loss/log_iter, streaming_accuracy/log_iter))\n",
        "\n",
        "                # save to log file for TensorBoard\n",
        "                summary_train = sess.run(train_summ_ops, feed_dict={streaming_loss_p: streaming_loss,\n",
        "                                                                    streaming_acc_p: streaming_accuracy})\n",
        "                writer.add_summary(summary_train, global_step=i)\n",
        "\n",
        "                streaming_loss = 0\n",
        "                streaming_accuracy = 0\n",
        "\n",
        "            if i % val_iter == 0:\n",
        "                saver.save(sess, os.path.join(checkpoint_path, 'checkpoint'), global_step=global_step)\n",
        "                print(\"Model saved!\")\n",
        "\n",
        "                sess.run(init_op_val)\n",
        "                validation_accuracy = 0\n",
        "                num_iter = 0\n",
        "                while True:\n",
        "                    try:\n",
        "                        acc_batch = sess.run(accuracy, feed_dict={training: False}) ##############################\n",
        "                        validation_accuracy += acc_batch\n",
        "                        num_iter += 1\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        validation_accuracy /= num_iter\n",
        "                        print(\"Iteration: {}, Validation accuracy: {:.2f}\".format(i, validation_accuracy))\n",
        "\n",
        "                        # save log file to TensorBoard\n",
        "                        summary_val = sess.run(val_summ_ops, feed_dict={val_acc_p: validation_accuracy})\n",
        "                        writer.add_summary(summary_val, global_step=i)\n",
        "\n",
        "                        sess.run(init_op_train) # switch back to training set\n",
        "                        break\n",
        "        writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDp8NcMqUwQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "fcdc1a7a-6298-47dc-fa5d-c920f3cfa844"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From <ipython-input-4-385b01cc5114>:6: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
            "WARNING:tensorflow:From <ipython-input-4-385b01cc5114>:6: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "WARNING:tensorflow:From <ipython-input-5-5e78e8b10cbd>:5: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-5-5e78e8b10cbd>:6: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-5-5e78e8b10cbd>:8: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-5e78e8b10cbd>:15: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-5e78e8b10cbd>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-5e78e8b10cbd>:18: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV2Fhk-HUzYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}